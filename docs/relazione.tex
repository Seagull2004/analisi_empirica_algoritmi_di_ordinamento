\documentclass[a4paper, 12pt, oneside]{book}
\usepackage[italian]{babel} % Lingua italiana
\usepackage{authblk} % Per gli autori
\usepackage{amsmath} % Per equazioni avanzate
\usepackage{amssymb} % Simboli matematici
\usepackage{graphicx} % Per immagini
\usepackage{lmodern} % Font simile a Computer Modern
\usepackage{geometry} % Per margini personalizzati
\usepackage{listings} % scrivere codice
\usepackage{float} % Per il posizionamento delle immagini
\geometry{a4paper, margin=2.5cm}
\cleardoublepage
\renewcommand\Authands{ e }
\renewcommand{\contentsname}{Contenuti}
\renewcommand{\chaptername}{Capitolo}

\title{Analisi empirica degli algoritmi di ordinamento}
\author[ ]{Graziano Francesco \thanks{Email: graziano.francesco@spes.uniud.it, Matricola: 166680}}
\author[ ]{Ongaro Michele \thanks{Email: ongaro.michele@spes.uniud.it, Matricola: 168049}}
\author[ ]{Petri Riccardo \thanks{Email: petri.riccardo@spes.uniud.it, Matricola: 167623}}
\author[ ]{Ungaro Marco \thanks{Email: ungaro.marco@spes.uniud.it, Matricola: 168934}}
\affil[ ]{Università degli Studi di Udine, Dipartimento di Scienze matematiche, informatiche e fisiche}
\date{A.A. 2024-2025}


\begin{document}

\maketitle % copertina del documento
\tableofcontents % sommario di tutti i titoli


\chapter{Introduzione}\label{chap:Introduzione}

Con la seguente relazione si vuole documentare i risultati empirici ottenuti dalla realizzazione e implementazione di quattro algoritmi di ordinamento per vettori di interi di dimensione variabile.
Gli algoritmi che andremo ad analizzare sono il Counting Sort, il Quick Sort, il Quick Sort 3 way e il Radix Sort.
Oltre alla corretta implementazione viene richiesto di effettuare un'analisi empirica dei tempi di esecuzione degli algorimti al variare della dimensione del vettore e del range dei valori interi. Per stimare i tempi di esecuzione di questi algoritmi garanantendo un errore relativo massimo pari a 0.001 adotteremo le seguenti metodologie:

\begin{itemize}
    \item Utilizzeremo un clock di sistema monotono per garantire precisione nelle misurazioni (ad esempio, \texttt{perf\_counter()} del modulo \(time\) in Python);
    \item Andremo a generare 100 campioni per ciascun grafico, con i valori dei parametri (dimensione del vettore\(n\) e intervallo dei valori \(m\)) distribuiti secondo una progressione geometrica;
    \item Effettueremo più esecuzioni per ogni campione, per stimare in modo affidabile il tempo medio di esecuzione.
\end{itemize}

\noindent Dopo aver stimato i tempi di esecuzione per ciascun algoritmo, risulterà interessante confrontare i grafici ottenuti per analizzarne il comportamento. 

% chapter Introduzione (end)


 
\chapter{Counting Sort}\label{chap:Counting Sort} % (fold)

% questa parte serve?
% da qui
Il Counting Sort è un algoritmo di ordinamento non comparativo: anziché effettuare confronti tra gli elementi, si basa sul conteggio delle occorrenze di ciascun elemento presente nel vettore da ordinare.
È particolarmente efficiente quando gli elementi da ordinare sono numeri interi compresi in un intervallo limitato (intervallo \([0, m]\)).
L'implementazione adottata si articola in tre fasi principali:

\begin{itemize}
    \item Conteggio delle occorrenze di ciascun elemento; si costruisce un vettore ausiliario \(C\) di lunghezza \(m+1\), inizializzato a zero, in cui per ogni elemento \(x\) in \(A\), si incrementa \(C[x]\) di 1.
    \item Calcolo delle posizioni cumulative; per ottenere la posizione finale di ciascun elemento nel vettore ordinato, si trasforma \(C\) in un vettore cumulativo. In questa versione, \(C[0]\) viene inizialmente decrementando di uno, così che \(c[i]\) rapprsenti l'indice massimo in cui si può inserire l'elemento \(i\) nel vettore ordinato.
    \item Costruzione del vettore ordinato; si scorre il vettore originale da destra a sinistra, e si inserisce ciascun elemento \(x\) nella posizione \(C[x]\) del vettore ordinato, decrementando \(C[x]\) di 1 dopo ogni inserimento. Questo garantisce la stabilità dell'ordinamento, poiché gli elementi con lo stesso valore vengono inseriti nell'ordine in cui appaiono nel vettore originale.
\end{itemize}
% a qui

\noindent Le due funzioni fornite nel codice sono:

\begin{itemize}
    \item \texttt{countingSort(A, B, m)}: modifica il vettore B scrivendoci all'interno l'ordinamento crescente di A;
    \item \texttt{uniformedCountingSort(A, m)}: funzione wrapper che restituisce direttamente una copia ordinata di A.
\end{itemize}

\section{Analisi della complessità}

% questa parte serve?
% da qui
Siano \(n\) la dimensione del vettore \(A\) e \(m\) il valore massimo contenuto in \(A\). \\

\noindent \textbf{Tempo:}

\begin{itemize}
    \item Conteggio: \(\Theta(n)\) per scorrere il vettore \(A\) e contare le occorrenze di ciascun elemento.
    \item Calcolo delle posizioni cumulative: \(\Theta(m)\) per trasformare il vettore \(C\) in un vettore cumulativo.
    \item Costruzione del vettore ordinato: \(\Theta(n)\) per scorrere il vettore \(A\) e inserire gli elementi nel vettore ordinato \(B\).
    \item Totale: \(\Theta(n + m)\).
\end{itemize}

\noindent \textbf{Spazio:}

\begin{itemize}
    \item Array ausiliario \(C\): richiede \(O(m)\) spazio.
    \item Array ordinato \(B\): richiede \(O(n)\) spazio.
    \item Totale: \(O(n + m)\).
\end{itemize}

\noindent L'algoritmo è efficiente quando \(m = O(n)\), ovvero quando la crescita dell'elemento massimo del vettore ha ordine inferiore o uguale alla crescita della lunghezza.
In scenari in cui \(m \gg n\), il costo della fase di conteggio e l'allocazione del vettore ausiliario \(C\) possono rendere l'algoritmo meno competitivo rispetto ad altri metodi di ordinamento.
% a qui


\section{Analisi empirica}

Sono stati condotti due esperimenti distinti per analizzare le prestazioni empiriche del Counting Sort:

\begin{itemize}
    \item Nel primo esperimento, la dimensione del vettore \(n\) varia, mentre il valore massimo \(m\) è mantenuto fisso a 1000000.
    \item Nel secondo esperimento, \(n\) è fissato a 10000, e viene fatto variare \(m\);
    \item Nel terzo esperimento: \(n\) varia tra 0 e 10000 e \(m\) è posto come il quadrato di \(n\)
\end{itemize}

\noindent In entrambi i casi, i parametri sono stati scelti seguendo una progressione geometrica.
Per ogni valore di \(n\) o \(m\) sono stati generati 100 campioni casuali, ciascuno eseguito più volte per stimare in modo affidabile il tempo medio di esecuzione, mantenendo un errore relativo massimo \(\leq\) 0.001.
Le misurazioni sono state effettuate utilizzando un clock monotono ad alta precisione (\texttt{time.perf\_counter()} in Python). \\

\subsection{Primo esperimento ($n$ varia)}

\noindent
In primo luogo, è stato analizzato l'andamento temporale al variare della dimensione del vettore \(n\), mantenendo costante l'intervallo dei valori a \(k = 100000\). 
Il grafico seguente mostra in:
\begin{itemize}
    \item \textbf{Ascissa}: dimensione \(n\) del vettore;
    \item \textbf{Ordinata}: tempo di esecuzione per l'ordinamento espresso in secondi.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_counting_sort_n.png}
    \caption{Performance del Counting Sort al variare di \(n\).}
    \label{fig:counting_sort_n}
\end{figure}

\subsection{Secondo esperimento ($m$ varia)}

\noindent Successivamente, è stato analizzato l'andamento temporale al variare dell'intervallo di valori \(m\), mantenendo costante la dimensione del vettore a \(n = 10000\). 
Il grafico seguente presenta:
\begin{itemize}
    \item \textbf{Ascissa}: intervallo \(m\) dei valori;
    \item \textbf{Ordinata}: tempo di esecuzione per l'ordinamento espresso in secondi.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_counting_sort_m.png}
    \caption{Performance del Counting Sort al variare di \(m\).}
    \label{fig:counting_sort_m}
\end{figure}

\subsection{Terzo esperimento (studio del caso peggiore)}
\noindent In terza battuta, al variare di \(n\) tra 100 e 10000 (per mantenere contenuto il tempo di esecuzione) è stato posto il massimo \(m\) come il quadrato della lunghezza del vettore. Con questa misurazione, mostriamo che se al crescere dell'input, l'elemento massimo cresce con un ordine superiore a quello della lunghezza, l'algoritmo diventa inefficiente rispetto ai tradizionali algoritmi basati su scambi e confronti, per i quali la crescita di \(m\) non comporta problemi. In questo caso l'andamento sembra essere proprio quadratico rispetto ad n (come ci aspettavamo) ma può arrivare ad essere anche peggio. In ogni caso i tempi anche solo per vettori da 10000 elementi risultano molto alti, superando gli 8 secondi, da qui vediamo anche le alte costanti moltiplicative nascoste dietro la notazione asintotica.
Il grafico seguente presenta:
\begin{itemize}
    \item \textbf{Ascissa}: intervallo \(n\) dei valori;
    \item \textbf{Ordinata}: tempo di esecuzione per l'ordinamento espresso in secondi.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/countingSortQuadratico.png}
    \caption{Performance del Counting Sort al variare di \(n\) e \(m\) = \(n\)^2.}
    \label{fig:counting_sort_m}
\end{figure}

% chapter Counting Sort (end)



\chapter{Quick Sort}\label{chap:Quick Sort} % (fold)

% serve davvero?
% da qui
Il Quick Sort è un algoritmo di ordinamento basato sulla strategia del divide et impera (ovvero: dividere il problema in sottoproblemi più piccoli, risolverli ricorsivamente e combinare i risultati).
A partire da un vettore \(A\), si seleziona un pivot, in questo caso l'elemento finale del sottovettore \(A[p...q]\), e si procede alla partizione: tutti gli elementi minori o uguali al pivot vengono spostati a sinistra, quelli maggiori a destra. L'indice finale del pivot è restituito dalla funzione partition, e l'algoritmo viene poi richiamato ricorsivamente sulle due sottosequenze risultanti.
% a qui?

\noindent Le due funzioni fornite nel codice sono:

\begin{itemize}
    \item \texttt{quicksort(A, p, q)}: aggiorna il contenuto del vettore A ordinandolo dal range da $p$ a $q$;
    \item \texttt{uniformedQuickSort(A, m)}: funzione wrapper per rendere uniforme l'interfaccia tra gli algoritmi testati che esegue l'algoritmo Quick Sort sull'intero vettore A.
\end{itemize}

\noindent
L'implementazione del Quick Sort analizzata non è in place a causa della coda delle ricorsioni, e non soddisfa la condizione di stabilità.

\section{Analisi della complessità}

Sia \(n\) la dimensione del vettore da ordinare:

\begin{itemize}
    \item Caso medio: \(O(n \cdot log(n))\), si verifica quando il pivot divide il vettore in modo bilanciato.
    \item Caso migliore: \(O(n \cdot log(n))\), con partizioni esattamente simmetriche.
    \item Caso peggiore: \(O(n^2)\), quando il pivot è sempre il minimo o il massimo elemento (partizione altamente sbilanciata).
    \item Spazio ausiliario: \(O(log(n))\) nel caso medio per la profondità dello stack ricorsivo; \(O(n)\) nel caso peggiore.
\end{itemize}

\noindent A differenza di altri algoritmi come Counting Sort o Radix Sort, Quick Sort non richiede conoscenza del range dei valori interi e lavora unicamente tramite confronti tra elementi.

\section{Analisi empirica}

Per analizzare empiricamente il comportamento del Quick Sort sono stati eseguiti due esperimenti distinti:

\begin{itemize}
    \item Nel primo esperimento la dimensione del vettore \(n\) viene fatta variare, mantenendo fisso \(m = 100000\).
    \item Nel secondo esperimento abbiamo \(n = 10000\) costante, con variazione di \(m\).
    \item Studio del caso peggiore: vettore ordinato, \(m = 100000\), \(n\) varia tra 100 e 10000
\end{itemize}

\noindent In entrambi i casi, i parametri sono distribuiti secondo una progressione geometrica.
Per ogni coppia \((n, m)\) sono stati generati 100 campioni casuali, ciascuno eseguito più volte per stimare in modo accurato il tempo medio di esecuzione, con un errore relativo massimo \(\leq 0.001\).
Le misurazioni sono state effettuate mediante un clock monotono ad alta precisione (\texttt{time.perf\_counter()} in Python). \\

\noindent
Poiché Quick Sort opera esclusivamente tramite confronti, il valore massimo m non influisce direttamente sulla complessità. Tuttavia, può avere effetti indiretti sulla distribuzione dei dati (ad esempio, la presenza di molti duplicati o valori ripetuti), il che può influenzare l'efficienza della partizione. \\

\subsection{Primo esperimento ($n$ varia)}

A partire dai dati misurati è stato generato un grafico nel quale la lunghezza del vettore indicata con \(n\) varia da 100 a 1000000, mentre il parametro \(m\) (range dei valori) è mantenuto costante. L'ordinamento è stato eseguito utilizzando l'algoritmo Quick Sort su vettori contenenti numeri casuali.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_quick_sort_n.png}
    \caption{Performance del Quick Sort al variare di \(n\).}
    \label{fig:quick_sort_n}
\end{figure}

\subsection{Secondo esperimento ($m$ varia)}

\noindent Successivamente, è stato realizzato un grafico in cui varia il range dei valori interi presenti nel vettore, indicato con \(m\), mantenendo costante la dimensione del vettore a \(n=10000\). Nel grafico riportato di seguito, sull'asse delle ascisse è rappresentata la variazione di \(m\), da 10 a 1.000.000, in scala scientifica. Sull'asse delle ordinate è riportato il tempo medio di esecuzione dell'ordinamento, espresso in secondi.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_quick_sort_m.png}
    \caption{Performance del Quick Sort al variare di \(m\).}
    \label{fig:quick_sort_m}
\end{figure}

\noindent Si osserva un'anomalia nel grafico quando il numero di valori distinti presenti nel vettore (\(m\)) è molto basso. La motivazione è presto detta: Quick Sort rischia di diventare $O(n^2)$ se ci sono tante ripetizioni dello stesso elemento, cosa che, per il principio della piccionaia avviene per valori di $m$ piuttosto bassi (e.g. un vettore da 10000 elementi che contiene elementi che possono assumere solamente 10 valori diversi risulta avere molte ripetizioni).

\subsection{Terzo esperimento (studio del caso peggiore)}

Per ottenere dei campioni su cui Quick Sort risulta pessimo, le misurazioni sono state effettuate generando vettori già ordinati, in modo da avere una cattiva scelta dell'elemento pivot. Similmente al grafico del terzo esperimento fatto su Counting Sort, vediamo che l'andamento sembra quadratico, proprio come ci aspettavamo. In ogni caso i tempi sono abbastanza contenuti rispetto al Counting Sort nel suo caso quadratico, dato che con vettori da 10000 elementi rimaniamo comunque sotto i 4 secondi. In questo caso n varia da 100 a 10000, anche con questa limitazione è stato necessario aumentare il limite di ricorsioni imposto di default da python.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/quickSortCasoPeggiore.png}
    \caption{Performance del Quick Sort al variare di \(n\) su vettori ordinati.}
    \label{fig:quick_sort_n}
\end{figure}

% chapter Quick Sort (end)



\chapter{Quick Sort 3 Way}\label{chap:Quick Sort 3 Way} % (fold)

Il Quick Sort 3-Way è una variante del Quick Sort classico, progettata per migliorare l'efficienza in presenza di molti elementi duplicati. Come il Quick Sort tradizionale, segue la strategia del \textit{divide et impera} (cioè suddividere il vettore in parti più semplici da ordinare ricorsivamente).
A differenza della versione classica, che suddivide in due partizioni (elementi minori o maggiori del pivot), il Quick Sort 3-Way suddivide il vettore in tre sezioni distinte:

\begin{itemize}
    \item Elementi minori del pivot;
    \item Elementi uguali al pivot;
    \item Elementi maggiori del pivot.
\end{itemize}

\noindent Questa suddivisione viene effettuata durante la fase di partizionamento, evitando confronti e ricorsioni inutili sugli elementi uguali al pivot, che sono già in posizione corretta. La ricorsione viene infatti applicata solo alle sottosequenze strettamente minori o maggiori del pivot.
L'algoritmo non è in-place per via della coda delle chiamate ricorsive e non è neanche stabile, infatti gli elementi uguali possono cambiare ordine relativo.

\section{Analisi della complessità}

Sia \(n\) la dimensione del vettore:

\begin{itemize}
    \item Caso migliore: \(O(n)\), quando tutti gli elementi sono uguali, e la partizione centrale include l'intero vettore.
    \item Caso medio: \(O(n \cdot log(n))\), come il Quick Sort classico, ma con una costante migliore grazie alla gestione efficiente dei duplicati.
    \item Caso peggiore: \(O(n^2)\), in caso di pivot molto sbilanciati e assenza di duplicati.
    \item Spazio ausiliario: \(O(log(n))\) per la profondità della ricorsione.
\end{itemize}

\noindent Nel complesso, Quick Sort 3-Way si comporta meglio del Quick Sort classico in presenza di dati con molti valori ripetuti, mantenendo lo stesso ordine di complessità.

\section{Analisi empirica}

Per verificare empiricamente l'efficienza dell'algoritmo, sono stati condotti tre esperimenti separati:

\begin{itemize}
    \item Nel primo esperimento abbiamo \(m = 100000\) fisso, mentre la dimensione del vettore che varia da \(100\) a $1000000$
    \item Nel secondo esperimento la dimensione del vettore è \(n = 10000\) mentre viene fatto variare il range dei valori \(m\) da $10$ a $1000000$.
    \item Nel terzo esperimento l'elemento massimo è \(n = 100000\) mentre viene fatto variare il range dei valori \(n\) da $100$ a $100000$ e i vettori generati sono tutti generati già ordinati, come per il caso peggiore del Quick Sort di base.
\end{itemize}

\noindent
I valori di \(n\) e \(m\) sono distribuiti secondo una progressione geometrica. Per ogni coppia di parametri sono stati generati 100 campioni casuali, ciascuno eseguito più volte. I tempi medi di esecuzione sono stati stimati con un clock monotono ad alta precisione (\texttt{time.perf\_counter()}), garantendo un errore relativo massimo \(\leq 0.001\). \\


\noindent
Nel grafico allegato si evidenziano chiaramente i vantaggi di questa variante rispetto alla versione classica, soprattutto in presenza di dati ridondanti.

\subsection{Primo esperimento ($n$ varia)}

Nel primo esperimento, i tempi crescono approssimativamente secondo una curva \(O(n \cdot log(n))\), come previsto.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_quick_sort_3_way_n.png}
    \caption{Performance del Quick Sort 3 Way al variare di \(n\).}
    \label{fig:quick_sort_3_way_n}
\end{figure}



\subsection{Secondo esperimento ($m$ varia)}

Nel secondo esperimento, si osserva che, riducendo \(m\), aumentano i duplicati nei dati, e l'algoritmo ne beneficia: i tempi di esecuzione diminuiscono sensibilmente grazie al partizionamento ottimizzato del Quick Sort 3-Way.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_quick_sort_3_way_m.png}
    \caption{Performance del Quick Sort 3 Way al variare di \(m\).}
    \label{fig:quick_sort_3_way_m}
\end{figure}

\subsection{Terzo esperimento (vettori ordinati)}

Notiamo che anche in questo caso, come per il Quick Sort base, l'andamento peggiora, ma sembra resistere meglio.
La performance risulta migliore grazie alla divisione in 3 parti del vettore, che probabilmente permette di evitare ricorsioni (anche se le ricorsioni evitate sono mediamente su un singolo elemento, il pivot).
In questo caso il range di \(n\) varia tra 100 e 100000 (fermandoci a 1000000 il tempo di misurazione e il numero di ricorsioni arrivava ad essere troppo elevato). In ogni caso vediamo che per vettori di 100000 elementi il tempo resta tra i 3 e 4 secondi, mentre questo avveniva con Quick Sort di base con vettori di appena 10000 elementi! (sempre con vettori ordinati)

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/quicksort3wayCasoPeggiore.png}
    \caption{Performance del Quick Sort 3 Way su vettori ordinati.}
    \label{fig:quick_sort_3_way_m}
\end{figure}

% chapter Quick Sort 3 Way (end)



\chapter{Radix Sort}\label{chap:Radix Sort} % (fold)

Il Radix Sort è un algoritmo di ordinamento non comparativo che sfrutta le rappresentazioni numeriche degli elementi. Ordina i numeri interi cifra per cifra, a partire dalla cifra meno significativa (LSD - Least Significant Digit) fino a quella più significativa. L'ordinamento delle singole cifre è effettuato tramite un algoritmo stabile, come il Counting Sort, applicato in sequenza per ogni posizione.
Radix Sort lavora bene quando si conosce il numero massimo di cifre degli elementi (ovvero \(log_{10}(k)\) se \(k\) è il valore massimo presente).
Questo algoritmo è in-place (usa spazio aggiuntivo proporzionale a \(n + b\), con \(b\) la base usata, tipicamente \(10\)), ma non stabile per natura a meno che il sorting delle cifre lo sia (in questo caso è garantita la stabilità tramite Counting Sort).

\section{Analisi della complessità}
Sia \(n\) la lunghezza del vettore e \(d\) il numero massimo di cifre (ossia \( d = \lceil \log_{10}(k + 1) \rceil \)): \\

\begin{itemize}
    \item Tempo: \(\Theta(d \cdot (n + b))\), che nel caso più comune (\(b = 10\)) diventa \(\Theta(n \cdot log_{10}(k))\). Quando \(k = O(n^c)\) per una costante \(c\), si ha complessivamente tempo quasi-lineare \(\Theta(n)\).
    \item Spazio: \(\Theta(n + b)\) per ogni cifra, dove \(b\) è la base (es. 10 cifre decimali).
    \item Stabile: sì, se il counting sort interno è stabile.
    \item In-place: no, nel senso stretto (usa memoria ausiliaria \(\Theta(n)\)).
\end{itemize}

\section{Analisi empirica}

L'algoritmo è stato testato su vettori di interi generati casualmente, secondo tre configurazioni sperimentali:

\begin{itemize}
    \item Nel primo esperimento \(m = 100000\) (valore massimo) fisso, variando la dimensione \(n\) del vettore da 100 a 1000000;
    \item Nel secondo esperimento \(n = 10000\) fisso, variando \(m\), cioè il massimo valore presente negli elementi da 10 a 1000000.
    \item Nel terzo esperimento è stato studiato lo stesso caso "pessimo" visto per il Counting Sort, ma dal momento che con \(m\) che cresce come il quadrato di \(n\) la complessità temporale di Radix Sort risulta \(\Theta(nlogn)\), abbiamo spinto \(n\) fino a 1000000, come negli gli altri casi.
\end{itemize}

\noindent In entrambi i casi, i parametri sono scelti su una scala geometrica. Ogni configurazione \((n, m)\) è stata eseguita su 100 campioni, ciascuno ripetuto più volte. I tempi di esecuzione sono stati misurati tramite un clock monotono con errore relativo massimo \(\leq 0.001\). \\

\noindent I risultati ottenuti confermano l'efficienza teorica:

\subsection{Primo esperimento ($n$ varia)}

Nel primo eseperimento osserviamo che il tempo di esecuzione cresce linearmente con \(n\), confermando il comportamento \(O(n \cdot log(m))\), dato \(m\) fisso.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_radix_sort_n.png}
    \caption{Performance del Radix Sort al variare di \(n\).}
    \label{fig:radix_sort_way_n}
\end{figure}

\subsection{Secondo esperimento ($m$ varia)}

Nel secondo esperimento, aumentando \(m\) (cioè il numero massimo di cifre \(d\)), si osserva un aumento graduale dei tempi dovuto alla crescita del numero di passaggi di Counting Sort, coerente con la complessità \(O(d \cdot n)\).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/grafico_radix_sort_m.png}
    \caption{Performance del Radix Sort variare di \(m\).}
    \label{fig:radix_sort_3_way_m}
\end{figure}

\subsection{Terzo esperimento ($m$ = $n^2$)}

Nel terzo esperimento vediamo che, sebbene Radix Sort impieghi più tempo di Counting Sort quando \(m\) è costante, quando facciamo aumentare \(m\) come \(n^2\) le parti si invertono drasticamente, visto che abbiamo un \(\Theta(nlogn)\) contro un \(\Theta(n^2)\).
Vediamo che anche con vettori da 1000000 elementi Radix Sort rimane sotto i 6 secondi, mentre appena con vettori da 10000 elementi counting Sort supera gli 8 secondi. Comunque quando \(m\) cresce in questo modo, rimangono migliori algoritmi \(O(nlogn)\) basati su scambi e confronti, come il Quick Sort.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/radixsort_nlogn.png}
    \caption{Performance del Radix Sort con \(m = n^2\).}
    \label{fig:radix_sort_3_way_m}
\end{figure}

\noindent Radix Sort si dimostra particolarmente adatto per dataset numerici con intervallo limitato, risultando spesso più veloce di algoritmi comparativi come Quick Sort nei casi in cui \(k = O(n)\) o \(k \gg n^2\).

% chapter Radix Sort (end)



\chapter{Conclusioni}\label{chap:Conclusioni} % (fold)

\begin{itemize}
    \item mostriamo che asintoticamente con n fissato e un m che cresce sono meglio gli algoritmi basati su sbambi e confronti (grafici con tutti gli algorimti)
    \item mostriamo che asintoticamente con m fissato e un n che cresce sono meglio gli algoritmi lineari se le loro ipotesi sono soddisfatte
    \item (opzionale) cosa succede se facciamo variare m e n conteporaneamente?
    \item (opzionale) volendo potremmo aggiungere le immagini quicksort_vs_3way.png (che confronta quicksort e quicksort3way nel caso di vettori ordinati) e radixsort_vs_countingsort_crescita_m_quadratica.png
\end{itemize}

// grafico complessivo qui

% chapter Conclusioni (end)



\end{document}
